# -*- coding: utf-8 -*-
"""XXXXXXXXXXXX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1besMJ6Wq3xFbp8_RZpI14_oB1MbKopPE
"""





"""https://github.com/facebookresearch/large_concept_model/tree/main"""

!git clone https://github.com/facebookresearch/large_concept_model.git



# Commented out IPython magic to ensure Python compatibility.
# %cd large_concept_model

!uv sync --extra cpu --extra eval --extra data

!uv pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu
!uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
!pip install -e ".[data,eval]"

pip install --upgrade pip
pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
pip install -e ".[data,eval]"

!uv run --extra data prepare_evaluation_data.py prepare_data \
    --dataset_name=cnn_dailymail \
    --output_dir=jsonl_dataset \
    --source_text_column=article \
    --target_text_column=highlights \
    --version=3.0.0 \
    --prompt_prefix="Summarize the following news to a concise list of highlights.\n[Text Start]:\n"\
    --prompt_suffix="\n[Text End]"

launcher = Launcher(
      cache=None,
      config_dump_dir=Path(log_dir) / "conf",
      log_folder=Path(log_dir) / "logs",
      cluster=mode,
      update_parameters={"partition": "your_slurm_partition"},
  )
_ = await launcher.schedule(inst_stopes_module)

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor llama3  \
  --model_name meta-llama/Llama-3.1-8B-Instruct \
  --generator_batch_size 16 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 200}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 16 \
  --dataset.soure_text_column prompt \
  --dataset.source_target_column answer \
  --dump_dir output_results

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor llama3  \
  --model_name meta-llama/Llama-3.2-1B-Instruct \
  --generator_batch_size 16 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 200}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 16 \
  --dataset.soure_text_column prompt \
  --dataset.source_target_column answer \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd examples/evaluation

!python /content/large_concept_model/examples/evaluation/prepare_evaluation_data.py --config c/content/large_concept_model/examples/evaluation/instruction.yaml

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

!pip install -e ".[data,eval]"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm
!uv run /content/large_concept_model/lcm/datasets/base.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/evaluation
!uv run /content/large_concept_model/lcm/evaluation/run.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/inference/lcm
!uv run /content/large_concept_model/lcm/inference/lcm/generator.py

!!uv run generator.py --prompt "dog" --output_dir "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/examples/evaluation
!uv run /content/large_concept_model/examples/evaluation/prepare_evaluation_data.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/inference/lcm
!uv run /content/large_concept_model/lcm/inference/lcm/generator.py -h

# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
#

from dataclasses import dataclass
from typing import List, Optional, Tuple

import torch
from fairseq2.generation.generator import (
    GenerationCounters,
    Hypothesis,
    SequenceGeneratorOutput,
)
from fairseq2.logging import get_log_writer

from lcm.datasets.batch import EmbeddingsBatch, PaddingMask
from lcm.models.abstract_lcm import AbstractLCModel
from lcm.nn.incremental_state import LCMIncrementalStateBag

logger = get_log_writer(__name__)


"""
This generator follows the style of existing generators in Fairseq2
"""


@dataclass
class LCMGeneratorOptions:
    """Holds the options to pass to a sequence generator."""

    max_seq_len: int = 200
    """The hard limit on maximum length of generated sequences."""

    min_seq_len: int = 1
    """The minimum length of generated sequences."""

    eos_threshold: Optional[float] = 0.9
    """Threshold for cosine similarity to the EOS vector"""

    sample_latent_variable: bool = True
    """When using VAE models, whether to return the mean or sample"""

    stop_on_repetition_cosine_threshold: Optional[float] = None
    """Stop the generation when the similarity of two consecutive concepts is above the threshold."""

    include_eos_token: bool = False
    """Whether the eos token should be included in the hypotheses (matters only if they are trimmed)."""

    trim_hypotheses: bool = False
    """Whether the tokens after the EOS token should be included in the hypotheses."""

    seed: Optional[int] = None
    """Seed to make generation deterministic"""

    lcm_temperature: float = 1.0
    """Temperature for decoding in the LCM"""


class LCMGenerator:
    """Generates with an LCM model."""

    def __init__(
        self,
        model: AbstractLCModel,
        options: Optional[LCMGeneratorOptions] = None,
        eos_vec: Optional[torch.Tensor] = None,
    ) -> None:
        """
        :param model:
            The LC model to use for generation.
        """
        model.eval()
        self.model = model

        if options is None:
            options = LCMGeneratorOptions()

        self.eos_vec = eos_vec
        if self.eos_vec is None and options.eos_threshold:
            logger.warning(
                f"eos_threshold is set to {options.eos_threshold}, but eos_vec is not provided"
            )
        if options.eos_threshold:
            logger.debug(f"The eos_vec in generator has been set to {self.eos_vec}")

        self.options = options

        self.max_seq_len = options.max_seq_len
        self.min_seq_len = options.min_seq_len

        assert self.min_seq_len >= 1, (
            f"min_seq_len must be greater than or equal to 1, min_seq_len={options.min_seq_len}"
        )

        self.eos_threshold = options.eos_threshold

        self.seqs: torch.Tensor
        self.step_nr = 0
        self.min_prompt_len: int
        self.max_prompt_len: int
        self.sample_indices: torch.Tensor
        self.state_bag: Optional[LCMIncrementalStateBag] = None
        self.prompt_seq_lens: Optional[torch.Tensor] = None
        self.prompt_padding_mask: Optional[torch.Tensor] = None
        self.lengths: torch.Tensor
        self.step_scores: torch.Tensor

    @torch.inference_mode()
    def __call__(
        self,
        batch_input: EmbeddingsBatch,
        max_gen_len: Optional[int] = None,
        min_gen_len: Optional[int] = None,
        temperature: float = 0.0,
        disable_cache: bool = False,
        **kwargs,
    ) -> SequenceGeneratorOutput:
        """
        :param input:
            `bacth_input` embedded and padded tensor sequence of the inputs
            `max_gen_len` max length to be generated for the given input
            `min_gen_len` minimum length to be generated for the given input
            `temperature` temperature to control the generation
            `disable_cache` if True, do not use kv-caching
        :returns:
            The output of the LCM generator, consists of :math:`N` lists of
            hypotheses for :math:`N` prompts. Each list has 1 Hypothesis
            (beam size = 1), of which `seq` has the  *Shape:* math:`(S+T, D)`
            (:math:`S` is the prompt length, :math:`T` the length of the
            generated sequence after the prompt and :math:`D` the model
            dimension.)

        """
        if self.options.seed:
            torch.manual_seed(self.options.seed)

        # Setup the variables
        batch_size, self.max_prompt_len, embed_dim = batch_input.seqs.size()
        prompt_padding_mask = batch_input.padding_mask
        if prompt_padding_mask is None:
            self.min_prompt_len = self.max_prompt_len
            self.prompt_padding_mask = None
            self.prompt_seq_lens = None
        else:
            self.prompt_seq_lens = prompt_padding_mask.seq_lens
            assert self.prompt_seq_lens is not None, (
                "Expecting a valid `self.prompt_seq_lens` Tensor, found `None`"
            )
            self.min_prompt_len = int(torch.min(self.prompt_seq_lens, dim=0)[0].item())

            # Keep the materialized mask
            self.prompt_padding_mask = prompt_padding_mask.materialize()

        if not max_gen_len:
            max_gen_len = self.max_seq_len

        # Make sure we do not accidentally set a max_gen_len that exceeds
        # the generator's model capability
        assert max_gen_len <= self.max_seq_len, (
            f"Generator can generate up to {self.max_seq_len} sequences, max_gen_len={max_gen_len}"
        )
        self.max_gen_len = max_gen_len

        if not min_gen_len:
            min_gen_len = self.min_seq_len

        assert min_gen_len > 0, (
            f"min_gen_len must be greater than or equal to 1, min_gen_len={min_gen_len}"
        )
        self.min_gen_len = min_gen_len

        if temperature == 0.0:
            # If the call doesn't pass a specific temperature,
            # use the default one from the decoding options
            temperature = self.options.lcm_temperature

        self.temperature = temperature

        for k, v in kwargs.items():
            if hasattr(self.options, k) and v:
                setattr(self.options, k, v)

        # Holds the generated sequences, scores and sample-dependent variables
        dtype = self.model.dtype
        device = batch_input.seqs.device

        if disable_cache:
            self.state_bag = None
        else:
            self.state_bag = LCMIncrementalStateBag(
                self.max_prompt_len + self.max_gen_len
            )

        # reserving full sequences capacity
        self.seqs = torch.zeros(
            (batch_size, self.max_prompt_len + self.max_gen_len, embed_dim),
            device=device,
            dtype=dtype,
        )
        self.step_scores = torch.zeros(
            (batch_size, self.max_prompt_len + self.max_gen_len),
            device=device,
        )
        self.lengths = torch.zeros(batch_size, dtype=torch.int, device=device) - 1

        # Hold the samples indices to return in order
        self.sample_indices = torch.arange(batch_size, device=device)
        # Output buffer
        self.hypotheses: List[List[Hypothesis]] = [[] for _ in range(batch_size)]

        # Bootstrap the sequences with the provided prompt.
        self.seqs[:, : self.max_prompt_len] = batch_input.seqs[:, : self.max_prompt_len]
        self.step_nr = self.min_prompt_len
        self.prefill(**kwargs)

        for self.step_nr in range(
            self.min_prompt_len, self.max_prompt_len + self.max_gen_len
        ):
            if not self._step():
                break

        return SequenceGeneratorOutput(self.hypotheses, counters=GenerationCounters())

    @torch.inference_mode()
    def prefill(self, **kwargs) -> None:
        """The initial forward pass in the decoder with the prefix/prompt
        to populate the KV-cache"""

        if self.state_bag is None:
            return

        # Prefilling with -1 since the next call to step will use the last token in the prefix
        prefill_len = self.step_nr - 1

        if prefill_len > 0:
            _ = self._decode(
                self.seqs[:, :prefill_len],
                padding_mask=None,
            )
            self.state_bag.increment_step_nr(prefill_len)  # type: ignore
        else:
            logger.warning(
                f"Skipping prefill since only a context size of {self.step_nr} is provided in the prefix"
            )

    @torch.inference_mode()
    def _decode(
        self,
        seqs: torch.Tensor,
        padding_mask: Optional[PaddingMask],
        **kwargs,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        output = self.model.predict_next_sentence(
            EmbeddingsBatch(seqs, padding_mask),
            sample=self.options.sample_latent_variable,
            temperature=self.temperature,
            state_bag=self.state_bag,
            **kwargs,
        )

        # Dummy scores
        scores = torch.zeros(seqs.shape[:-1])
        return output.seqs, scores

    def _step(self) -> bool:
        # Generate the next step output.

        if self.state_bag is None:
            # Without a state_bag, we're forwarding the full prefix
            model_output, step_score = self._decode(
                seqs=self.seqs[:, : self.step_nr],
                padding_mask=None,
            )
        else:
            # Since we're using a state_bag, we're only forwarding the last embedding
            model_output, step_score = self._decode(
                seqs=self.seqs[:, self.step_nr - 1 : self.step_nr],
                padding_mask=None,
            )

            self.state_bag.increment_step_nr()

        # model_output: EmbeddingBag
        return self.finalize_step(model_output, step_score)

    def finalize_step(
        self, model_output: torch.Tensor, step_score: torch.Tensor
    ) -> bool:
        """Post-processing and finalizing a step
        by checking all stopping criteria
        Takes the model's outputed embeddings (model_output)
        and their associated scores (step_score)
        If we're stepping, return True, else return False
        """
        already_finished = self.lengths > -1
        should_finish_now = torch.zeros_like(already_finished)

        model_last_output = model_output[:, -1]
        device = model_last_output.device

        # Ignore prompt positions between min-max prompt_len
        must_keep_going = None
        if self.step_nr < self.max_prompt_len:
            assert self.prompt_padding_mask is not None, (
                f"If self.prompt_padding_mas is None, then self.step_nr should start from self.max_prompt_len={self.max_prompt_len} - currently self.step_nr = {self.step_nr}"
            )
            mask = self.prompt_padding_mask[:, self.step_nr]
            model_last_output[mask] = self.seqs[mask, self.step_nr]
            must_keep_going = mask

        # Check stopping based on EOS similarity.
        if self.eos_threshold is not None and self.eos_vec is not None:
            sim2eos = torch.nn.functional.cosine_similarity(
                self.eos_vec.to(device), model_last_output
            )
            logger.debug(f"Similarity to eos vector: {sim2eos} vs {self.eos_threshold}")
            should_finish_now = should_finish_now | sim2eos.ge(self.eos_threshold)

        # Check stopping based on repetition.
        if (
            self.options.stop_on_repetition_cosine_threshold is not None
            and self.step_nr > 0
        ):
            sim2prev = torch.nn.functional.cosine_similarity(
                self.seqs[:, self.step_nr - 1], model_last_output
            )
            logger.debug(
                f"Similarity to prev vector: {sim2prev} vs {self.options.stop_on_repetition_cosine_threshold}"
            )
            should_finish_now = should_finish_now | sim2prev.ge(
                self.options.stop_on_repetition_cosine_threshold
            )

        if must_keep_going is not None:
            logger.debug(
                f"Must keep going (to cover max_prompt_len={self.max_prompt_len}) is not None = {must_keep_going}"
            )
            should_finish_now = should_finish_now & ~must_keep_going

        # Keep going if output is shorter than min_gen_len:
        if self.prompt_seq_lens is not None:
            longer_than_min_gen_len = (self.step_nr - self.prompt_seq_lens).ge(
                self.min_gen_len
            )
        else:
            longer_than_min_gen_len = (
                self.step_nr - self.max_prompt_len
            ) >= self.min_gen_len

        logger.debug(
            f"Longer than min_gen_len ({self.min_gen_len}) = {longer_than_min_gen_len}"
        )
        should_finish_now = should_finish_now & longer_than_min_gen_len
        stopped_on_eos = should_finish_now

        # Stop hypotheses that reached max_gen_len
        if self.prompt_seq_lens is not None:
            exceeds_max_gen_len = (self.step_nr - self.prompt_seq_lens + 1).ge(
                self.max_gen_len
            )
            logger.debug(
                f"step: {self.step_nr}; max_gen_len: {self.max_gen_len}; promt_lens: {self.prompt_seq_lens}; steps exceeded: {self.max_gen_len + self.prompt_seq_lens}"
            )

        else:
            exceeds_max_gen_len = (
                self.step_nr - self.max_prompt_len + 1
            ) >= self.max_gen_len
            logger.debug(
                f"step: {self.step_nr}; max_gen_len: {self.max_gen_len}; promt_lens: None (unique length: {self.max_prompt_len}); steps exceeded: {self.max_prompt_len + self.max_gen_len}"
            )

        logger.debug(
            f"Stopping criteria: {should_finish_now}; exceeds max len: {exceeds_max_gen_len}; already finished: {already_finished}"
        )

        should_finish_now = should_finish_now | exceeds_max_gen_len

        # Assign lengths to the sequences that have just finished.
        should_finish_now = should_finish_now & ~already_finished
        self.lengths[should_finish_now] = self.step_nr + 1

        # Record the current step.
        self.seqs[:, self.step_nr] = model_last_output.squeeze(1)
        self.step_scores[:, self.step_nr - self.min_prompt_len] = step_score[:, -1]

        #  Save completed hypsptheses
        finished_mask = self.lengths.ne(-1)
        finished_indices = finished_mask.nonzero()

        # Remove finished hypotheses and reorder variables/state_bag if any are left
        if len(finished_indices) > 0:
            for idx in finished_indices:
                self.finish_sequence(int(idx), is_eos=bool(stopped_on_eos[int(idx)]))

        active_mask = ~finished_mask
        active_indices = active_mask.nonzero().squeeze(-1)

        if len(active_indices) == 0:
            return False

        self.reorder_state(active_indices)

        return True

    def finish_sequence(self, idx: int, is_eos: bool = False) -> None:
        seq_len = int(self.lengths[idx].item())

        if self.options.trim_hypotheses and self.lengths[idx].item() > -1 and is_eos:
            seq_len = int(self.lengths[idx].item()) - int(
                not self.options.include_eos_token
            )

        sample_idx = int(self.sample_indices[idx])
        self.hypotheses[sample_idx] = [
            Hypothesis(
                seq=self.seqs[idx, :seq_len],
                score=None,
                step_scores=self.step_scores[idx],  # Trim it as well?
            )
        ]

    def state_bag_reorder(self, new_order: torch.Tensor) -> None:
        if self.state_bag is not None:
            self.state_bag.reorder(new_order)

    def reorder_state(self, new_order: torch.Tensor) -> None:
        self.state_bag_reorder(new_order)

        self.seqs = self.seqs.index_select(dim=0, index=new_order)

        self.sample_indices = self.sample_indices.index_select(dim=0, index=new_order)

        self.step_scores = self.step_scores.index_select(dim=0, index=new_order)

        self.lengths = self.lengths.index_select(dim=0, index=new_order)

        if self.prompt_padding_mask is not None:
            self.prompt_padding_mask = self.prompt_padding_mask.index_select(
                dim=0, index=new_order
            )

        if self.prompt_seq_lens is not None:
            self.prompt_seq_lens = self.prompt_seq_lens.index_select(
                dim=0, index=new_order
            )

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/inference/lcm
!uv run generator.py -h

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/scripts
!uv run prepare_wikipedia.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/scripts
!uv run prepare_wikipedia.py OUTPUT_DIR /content

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/inference/two_tower_diffusion_lcm
!uv run generator.py --help

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/inference/two_tower_diffusion_lcm
!uv run generator.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model
!python -m lcm.train \
    +pretrain=mse \
    ++trainer.output_dir="checkpoints/mse_lcm" \
    ++trainer.experiment_name=training_mse_lcm \

!torchrun --standalone

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor llama3  \
  --model_name meta-llama/Llama-3.1-8B-Instruct \
  --generator_batch_size 16 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 200}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 16 \
  --dataset.soure_text_column prompt \
  --dataset.source_target_column answer \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor llama3  \
  --model_name meta-llama/Llama-3.1-8B-Instruct \
  --generator_batch_size 16 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 200}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 16 \
  --source_text_column prompt \
  --target_text_column answer \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/inference/lcm
!uv run generator.py --prompt "a beautiful landscape with trees and river" --output_dir "./my_images"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor llama3 \
  --model_name meta-llama/Llama-3.1-8B-Instruct \
  --generator_batch_size 16 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 200}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 16 \
  --dataset.source_text_column prompt \
  --dataset.target_text_column answer \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
!mkdir -p jsonl_dataset/cnn_dailymail

# ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
!python -m lcm.evaluation.tasks.prepare_cnn_dailymail \
  --output_dir jsonl_dataset/cnn_dailymail \
  --split test

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

# ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© datasets
#!pip install datasets

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Hugging Face
from datasets import load_dataset
import json
import os

dataset = load_dataset("cnn_dailymail", version="3.0.0", split="test")

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ø¬Ù„Ø¯ ÙˆØ§Ù„Ù…Ù„Ù
output_dir = "/content/large_concept_model/lcm/jsonl_dataset/cnn_dailymail"
output_file = os.path.join(output_dir, "test.jsonl")

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
os.makedirs(output_dir, exist_ok=True)

# Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
with open(output_file, "w") as f:
    for item in dataset:
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ…Ø§ ÙŠØªÙˆÙ‚Ø¹Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        data = {
            "prompt": f"Summarize: {item['article']}",
            "answer": item["highlights"]
        }
        f.write(json.dumps(data) + "\n")

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")

!pip install datasets

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
!mkdir -p jsonl_dataset/cnn_dailymail

# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«Ø§Ù„ (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ script Ù…Ø®ØµØµ)
!python -c "
import json
# Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
sample_data = [
    {'prompt': 'Summarize: This is a test article about technology.', 'answer': 'Technology test summary.'},
    {'prompt': 'Summarize: Another example for evaluation.', 'answer': 'Evaluation example summary.'}
]

with open('jsonl_dataset/cnn_dailymail/test.jsonl', 'w') as f:
    for item in sample_data:
        f.write(json.dumps(item) + '\\n')
print('ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù test.jsonl Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«Ø§Ù„')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor llama3 \
  --model_name meta-llama/Llama-3.2-1B-Instruct \
  --generator_batch_size 2 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 100}' \
  --dataset_dir /content/large_concept_model/lcm/jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 2 \
  --dump_dir output_results

!huggingface-cli login --token c

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¹Ø·ÙˆØ¨
!rm jsonl_dataset/cnn_dailymail/test.jsonl

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù JSONL ØµØ­ÙŠØ­
import json

# Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«Ø§Ù„ ØµØ­ÙŠØ­Ø©
sample_data = [
    {"prompt": "Summarize: This is a test article about technology advancements in 2024.", "answer": "Technology advancements in 2024 include AI and quantum computing."},
    {"prompt": "Summarize: Climate change effects are becoming more visible each year.", "answer": "Climate change impacts are increasing globally with rising temperatures."}
]

# ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
with open('jsonl_dataset/cnn_dailymail/test.jsonl', 'w') as f:
    for item in sample_data:
        f.write(json.dumps(item) + '\n')

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù test.jsonl Ø¬Ø¯ÙŠØ¯ Ø¨ØªÙ†Ø³ÙŠÙ‚ ØµØ­ÙŠØ­")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor huggingface \
  --model_name "gpt2" \
  --generator_batch_size 2 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 50}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 2 \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# Ø¥ØµÙ„Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
import json

# Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
sample_data = [
    {"article": "This is a test article about technology advancements in 2024.", "highlights": "Technology advancements in 2024 include AI and quantum computing."},
    {"article": "Climate change effects are becoming more visible each year.", "highlights": "Climate change impacts are increasing globally with rising temperatures."}
]

# ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
with open('jsonl_dataset/cnn_dailymail/test.jsonl', 'w') as f:
    for item in sample_data:
        f.write(json.dumps(item) + '\n')

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: article Ùˆ highlights")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor huggingface \
  --model_name "gpt2" \
  --generator_batch_size 1 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 50}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 1 \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!pip install datasets

import json
from datasets import load_dataset

# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©
dataset = load_dataset("cnn_dailymail", "3.0.0", split="test[:5]")  # 5 Ø¹ÙŠÙ†Ø§Øª ÙÙ‚Ø·

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
with open('jsonl_dataset/cnn_dailymail/test.jsonl', 'w') as f:
    for item in dataset:
        data = {
            "article": item["article"],
            "highlights": item["highlights"]
        }
        f.write(json.dumps(data) + '\n')

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# ØªØ«Ø¨ÙŠØª Ø¨ÙŠØ§Ù†Ø§Øª NLTK Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
import nltk
nltk.download('punkt_tab')
nltk.download('punkt')

"""### aØ´ØºØ§Ù„"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor huggingface \
  --model_name "gpt2" \
  --generator_batch_size 1 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 50}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 1 \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
!cat output_results/results/cnn_dailymail_llm.test.json

# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø´Ø£Ø©
!ls -la output_results/

"""O] Writing metric logs to output_results/metrics.eval.jsonl
ğŸ‰ Ù…Ø¨Ø±ÙˆÙƒ! Ù„Ù‚Ø¯ Ù†Ø¬Ø­Øª ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­!

Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ Ø­ØµÙ„Øª Ø¹Ù„ÙŠÙ‡Ø§:
Ù„Ù‚Ø¯ Ø¹Ù…Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ ÙˆÙ‡Ø§ Ù‡ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:

ğŸ“Š Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡:
Rouge-2: 0.089767 (Ù…Ù‚ÙŠØ§Ø³ Ù„Ù„ØªÙ„Ø®ÙŠØµ)

Rouge-L: 0.127350 (Ù…Ù‚ÙŠØ§Ø³ Ù„Ù„ØªÙ„Ø®ÙŠØµ)

Rouge-LSum: 0.127350 (Ù…Ù‚ÙŠØ§Ø³ Ù„Ù„ØªÙ„Ø®ÙŠØµ)

Ngram Overlap: 0.678012 (ØªØ¯Ø§Ø®Ù„ n-gram)

Repetition-4: 1.850553 (Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±)

ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§:
Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø®Ø§Ù…: output_results/raw_results/

Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³: output_results/results/cnn_dailymail_llm.test.json

Ø³Ø¬Ù„Ø§Øª Tensorboard: output_results/tb/

Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³: output_results/metrics.eval.jsonl

ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:
Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ¨ÙŠÙ† Ø£Ù† Ù†Ù…ÙˆØ°Ø¬ GPT-2 Ù‚Ø§Ù… Ø¨Ù€:

âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª CNN/DailyMail Ø¨Ù†Ø¬Ø§Ø­

âœ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ„Ø§Ø®ÙŠØµ Ù„Ù„articles

âœ… Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©


"""

# Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ«Ø±
!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor huggingface \
  --model_name "gpt2-medium" \
  --generator_batch_size 1 \
  --tasks cnn_dailymail_llm.test \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 1 \
  --dump_dir output_results_large

!python -m lcm.convert \
  --model_name "gpt2" \
  --output_dir "lcm_gpt2" \
  --device "cpu"

!uv run lcm.convert \
  --model_name "gpt2" \
  --output_dir "lcm_gpt2" \
  --device "cpu"













# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…ÙˆØ¬ÙˆØ¯ Ø¥Ù„Ù‰ LCM
python -m lcm.convert \
  --model_name "gpt2" \
  --output_dir "./converted_model" \
  --device "cpu"

from lcm import LCModel

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„
model = LCModel.from_pretrained("./converted_model")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
concepts = model.extract_concepts("Your text here")

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„
python -m lcm.evaluation \
  --predictor lcm \
  --model_path "./converted_model" \
  --tasks "concept_understanding_task"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø§Ø³ØªØ®Ø¯Ø§Ù… uv Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„
uv run python -m lcm.convert \
  --model_name "llama-3-8b" \
  --output_dir "lcm_model" \
  --concept_dim 256

# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ Ù…Ø¹ uv
!uv run --no-cache \
  -m lcm.convert \
  --model_name "llama-3-8b" \
  --output_dir "lcm_model" \
  --concept_dim 256 \
  --device "cpu"

uv run -m lcm.convert \
  --model_name "llama-3-8b" \
  --output_dir "lcm_model_output" \
  --concept_dim 512 \          # Ø­Ø¬Ù… Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
  --batch_size 4 \             # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
  --device "cuda" \            # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU
  --num_workers 2 \            # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø§Ù…Ù„ÙŠÙ†
  --max_length 2048 \          # Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰
  --save_format "safetensors"  # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø­ÙØ¸

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
!uv sync

# Ø«Ø§Ù†ÙŠØ§Ù‹: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
uv run -m lcm.convert \
  --model_name "gpt2" \          # Ø¨Ø¯Ø§ÙŠØ© Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ±
  --output_dir "./gpt2_lcm" \
  --concept_dim 128 \
  --device "cuda" \
  --batch_size 8

# Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£ØµØºØ± (Ø£Ø³Ø±Ø¹)
uv run -m lcm.convert \
  --model_name "microsoft/DialoGPT-small" \
  --output_dir "./dialogpt_lcm" \
  --concept_dim 192

# Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©
uv run -m lcm.convert \
  --model_name "gpt2-medium" \
  --output_dir "./gpt2_medium_lcm" \
  --concept_dim 256 \
  --device "cuda"

uv run -m lcm.convert \
  --model_name "gpt2" \
  --output_dir "./lcm_model" \
  --concept_dim 64 \      # ØªÙ‚Ù„ÙŠÙ„ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
  --batch_size 2 \        # ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
  --device "cpu"          # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±

uv run python -c "
from lcm import LCModel
model = LCModel.from_pretrained('./gpt2_lcm')
print('ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!')
print(f'Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {model.concept_dim}')
"

# Ù…Ø¹ ØªÙØ¹ÙŠÙ„ logging Ù…ÙØµÙ„
uv run -m lcm.convert \
  --model_name "gpt2" \
  --output_dir "./lcm_model" \
  --concept_dim 256 \
  --log_level "INFO"









# Commented out IPython magic to ensure Python compatibility.
Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… `uv` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† `python` Ù…Ø¨Ø§Ø´Ø±Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ LCMØŒ Ø¥Ù„ÙŠÙƒ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©:

## ğŸ”§ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… uv:

```bash
# %cd /content/large_concept_model

# Ø§Ø³ØªØ®Ø¯Ø§Ù… uv Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„
uv run python -m lcm.convert \
  --model_name "llama-3-8b" \
  --output_dir "lcm_model" \
  --concept_dim 256
```

## ğŸ’¡ Ø£Ùˆ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù…Ø¹ uv:

```bash
# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ Ù…Ø¹ uv
uv run --no-cache \
  -m lcm.convert \
  --model_name "llama-3-8b" \
  --output_dir "lcm_model" \
  --concept_dim 256 \
  --device "cuda"
```

## âš™ï¸ Ù…Ø¹Ù„Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù‡Ù…Ø©:

```bash
uv run -m lcm.convert \
  --model_name "llama-3-8b" \
  --output_dir "lcm_model_output" \
  --concept_dim 512 \          # Ø­Ø¬Ù… Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
  --batch_size 4 \             # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
  --device "cuda" \            # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU
  --num_workers 2 \            # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø§Ù…Ù„ÙŠÙ†
  --max_length 2048 \          # Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰
  --save_format "safetensors"  # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø­ÙØ¸
```

## ğŸš€ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ ÙƒØ§Ù…Ù„:

```bash
# %cd /content/large_concept_model

# Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
uv sync

# Ø«Ø§Ù†ÙŠØ§Ù‹: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
uv run -m lcm.convert \
  --model_name "gpt2" \          # Ø¨Ø¯Ø§ÙŠØ© Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ±
  --output_dir "./gpt2_lcm" \
  --concept_dim 128 \
  --device "cuda" \
  --batch_size 8
```

## ğŸ“ Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©:

```bash
# Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£ØµØºØ± (Ø£Ø³Ø±Ø¹)
uv run -m lcm.convert \
  --model_name "microsoft/DialoGPT-small" \
  --output_dir "./dialogpt_lcm" \
  --concept_dim 192

# Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©
uv run -m lcm.convert \
  --model_name "gpt2-medium" \
  --output_dir "./gpt2_medium_lcm" \
  --concept_dim 256 \
  --device "cuda"
```

## âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©:

1. **Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©**:
```bash
# Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
nvidia-smi
```

2. **Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ØºÙŠØ± ÙƒØ§ÙÙŠØ©**:
```bash
uv run -m lcm.convert \
  --model_name "gpt2" \
  --output_dir "./lcm_model" \
  --concept_dim 64 \      # ØªÙ‚Ù„ÙŠÙ„ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
  --batch_size 2 \        # ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
  --device "cpu"          # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
```

3. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„**:
```bash
uv run python -c "
from lcm import LCModel
model = LCModel.from_pretrained('./gpt2_lcm')
print('ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!')
print(f'Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {model.concept_dim}')
"
```

## ğŸ” Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„ØªÙ‚Ø¯Ù…:

```bash
# Ù…Ø¹ ØªÙØ¹ÙŠÙ„ logging Ù…ÙØµÙ„
uv run -m lcm.convert \
  --model_name "gpt2" \
  --output_dir "./lcm_model" \
  --concept_dim 256 \
  --log_level "INFO"
```

**Ø¬Ø±Ø¨ Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ÙˆØ³Ø£ÙƒÙˆÙ† Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡Øª Ø£ÙŠ Ù…Ø´Ø§ÙƒÙ„!**

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
!uv sync

# Ø«Ø§Ù†ÙŠØ§Ù‹: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
!uv run -m lcm.convert \
  --model_name "openai-community/gpt2" \          # Ø¨Ø¯Ø§ÙŠØ© Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ±
  --output_dir "/content/large_concept_model" \
  --concept_dim 128 \
  --device "cpu" \
  --batch_size 8



!pip install --upgrade torchvision

import nltk
nltk.download('all')



# Commented out IPython magic to ensure Python compatibility.
# %cd large_concept_model

!uv sync --extra cpu --extra eval --extra data

!uv pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu
!uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
!pip install -e ".[data,eval]"

!uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu

!pip install -e ".[data,eval]"







# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
!uv sync

# Ø«Ø§Ù†ÙŠØ§Ù‹: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
!uv run -m lcm.convert \
--model_name "openai-community/gpt2" \
--output_dir "/content/large_concept_model" \
--concept_dim 128 \
--device "cpu" \
--batch_size 8

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
!uv sync

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
!uv run -m lcm.convert \
  --model_name "openai-community/gpt2" \
  --output_dir "./gpt2_lcm_model" \
  --concept_dim 128 \
  --device "cpu" \
  --batch_size 8 \
  --max_length 512

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø´Ø£Ø©
!ls -la ./gpt2_lcm_model/

# Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
!uv run python -c "
from lcm import LCModel
try:
    model = LCModel.from_pretrained('./gpt2_lcm_model')
    print('âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!')
    print(f'Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {model.concept_dim}')
except Exception as e:
    print(f'âŒ Ø®Ø·Ø£: {e}')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# ØªØ«Ø¨ÙŠØª fairseq2
!pip install fairseq2

# Ø£Ùˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… uv
!uv add fairseq2

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model
# Ø¨Ø¹Ø¯ ØªØ«Ø¨ÙŠØª fairseq2ØŒ Ø¬Ø±Ø¨ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
!uv run -m lcm.convert \
--model_name "openai-community/gpt2" \
--output_dir "./gpt2_lcm_model" \
--concept_dim 128 \
--device "cpu" \
--batch_size 8







# Commented out IPython magic to ensure Python compatibility.
# Ø§Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠÙ‡ Ù…Ø´Ø§ÙƒÙ„
!rm -rf /content/large_concept_model

# Ø§Ø³ØªÙ†Ø³Ø§Ø® Ù…Ù† Ø¬Ø¯ÙŠØ¯
!git clone https://github.com/facebookresearch/large_concept_model.git
# %cd /content/large_concept_model

https://github.com/facebookresearch/large_concept_model.git

!git clone https://github.com/facebookresearch/large_concept_model.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø§Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„ØµØ­ÙŠØ­ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
!pip install -e .

# Ø£Ùˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… uv
!uv pip install -e .

!uv pip install -e .



!uv sync --extra cpu --extra eval --extra data

!uv pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu
!uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
!pip install -e ".[data,eval]"

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† lcm Ù…Ø«Ø¨Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
!python -c "import lcm; print('âœ… LCM Ù…Ø«Ø¨Øª Ø¨Ù†Ø¬Ø§Ø­'); print(dir(lcm))"

!python -m lcm.convert \
--model_name "openai-community/gpt2" \
--output_dir "./gpt2_lcm_model" \
--concept_dim 128 \
--device "cpu" \
--batch_size 8

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model
# Ø¨Ø¹Ø¯ ØªØ«Ø¨ÙŠØª fairseq2ØŒ Ø¬Ø±Ø¨ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
!uv run -m lcm.convert \
--model_name "openai-community/gpt2" \
--output_dir "./gpt2_lcm_model" \
--concept_dim 128 \
--device "cpu" \
--batch_size 8

import pandas as pd

dataset_path = "/content/large_concept_model/sample_data/0_a25e918a7789ecfa_0_0.parquet" # dataset path

df = pd.read_parquet(dataset_path) # load dataset in pandas df

df['split'] = 'train' # adding 'split' column to dataset because of the missing split column

df.to_parquet(dataset_path) # convert dataset to parquest again

"""https://github.com/facebookresearch/large_concept_model/issues/19"""

!torchrun --standalone --nnodes=1 --nproc-per-node=2 -m lcm.evaluation  \
  --predictor base_lcm --sample_latent_variable False \
  --model_card checkpoints/mse_lcm/checkpoints/step_2000/model_card.yaml \
  --launcher standalone \
  --dataset.parquet_path /mnt/large_concept_model/examples/evaluation/parquet_dataset/cnn_dailymail/0_3e1f58ddc7724a53_0_0.parquet \
  --dataset.source_column prompt_sentences_sonar_emb \
  --dataset.source_text_column prompt_sentences \
  --dataset.target_column answer_sentences_sonar_emb \
  --dataset.target_text_column prompt_sentences \
  --tasks lcm_generation \
  --task_args '{"max_gen_len": 200}' \
  --data_loading.batch_size 4  --generator_batch_size 4 \
  --dump_dir /mnt/large_concept_model/output

import torch
from fairseq2.gang import FakeGang
from lcm.datasets.configs import ParquetDatasetConfig, EvaluationDataLoadingConfig
from lcm.evaluation.utils.data_utils import ParquetTestDataLoader

dataset = ParquetDatasetConfig(
    parquet_path="YOUR Parquet file",
    source_column="prompt_sentences_sonar_emb",
    source_text_column="prompt_sentences",
    target_column="answer_sentences_sonar_emb",
    target_text_column= "answer_sentences"
)

data_loading = EvaluationDataLoadingConfig(batch_size=1, seed=23, min_length_of_sequences=1, nb_epochs=1)

data_loader = ParquetTestDataLoader(
    data_config=data_loading,
    datasets=[dataset],
    gang=FakeGang(device=torch.device("cuda:0")),
    dtype=torch.float32,
)

cnt = 0
for batch in data_loader.iterate_batches():
    cnt += len(batch)

!torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation  \
  --predictor base_lcm \
  --model_card /content/drive/MyDrive/LCM/checkpoints/mse_lcm/checkpoints/step_10000/model_card.yaml \
  --launcher standalone \
  --dataset.parquet_path /content/drive/MyDrive/LCM/eval_data/0_55ac997a0bfaa427_0_0.parquet \
  --dataset.source_column prompt_sentences_sonar_emb \
  --dataset.source_text_column prompt_sentences \
  --dataset.target_column answer_sentences_sonar_emb \
  --dataset.target_text_column prompt_sentences \
  --tasks lcm_generation \
  --task_args '{"max_gen_len": 200}' \
  --data_loading.batch_size 16  --generator_batch_size 16 \
  --dump_dir /content/drive/MyDrive/LCM/output_results_lcm \

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ LCM
!uv run /content/large_concept_model/lcm/train/lcm/trainer.py \
  --model_name "gpt2" \
  --output_dir "./lcm_model" \
  --dataset "/content/large_concept_model/lcm/jsonl_dataset"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ LCM Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
!python -m lcm.inference.generate \
  --model_path "./path_to_lcm_model" \
  --prompt "Your text here"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/examples

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ØªÙˆÙØ±Ø©
!python basic_usage.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

import json
import os

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
os.makedirs('jsonl_dataset/cnn_dailymail', exist_ok=True)

# Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
sample_data = [
    {"article": "This is a test article about technology advancements in 2024.", "highlights": "Technology advancements in 2024 include AI and quantum computing."},
    {"article": "Climate change effects are becoming more visible each year.", "highlights": "Climate change impacts are increasing globally with rising temperatures."}
]

# ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
with open('jsonl_dataset/cnn_dailymail/test.jsonl', 'w') as f:
    for item in sample_data:
        f.write(json.dumps(item) + '\n')

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: article Ùˆ highlights")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù
!ls -la jsonl_dataset/cnn_dailymail/

python -m lcm.train \
    +finetune=two_tower \
    ++trainer.output_dir="checkpoints/finetune_two_tower_lcm" \
    ++trainer.experiment_name=finetune_two_tower_lcm \
    ++trainer.model_config_or_name=my_pretrained_two_tower

!uv run lcm.train \
    +finetune=two_tower \
    ++trainer.output_dir="checkpoints/finetune_two_tower_lcm" \
    ++trainer.experiment_name=finetune_two_tower_lcm \
    ++trainer.model_config_or_name=my_pretrained_two_tower







# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

import json
import os

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
os.makedirs('jsonl_dataset/cnn_dailymail', exist_ok=True)

# Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
sample_data = [
    {"article": "This is a test article about technology advancements in 2024.", "highlights": "Technology advancements in 2024 include AI and quantum computing."},
    {"article": "Climate change effects are becoming more visible each year.", "highlights": "Climate change impacts are increasing globally with rising temperatures."}
]

# ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØµØ­ÙŠØ­Ø©
with open('jsonl_dataset/cnn_dailymail/test.jsonl', 'w') as f:
    for item in sample_data:
        f.write(json.dumps(item) + '\n')

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: article Ùˆ highlights")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù
!ls -la jsonl_dataset/cnn_dailymail/

"""### ayhgØ´ØºØ§Ù„"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation \
  --predictor huggingface \
  --model_name "gpt2" \
  --generator_batch_size 1 \
  --tasks cnn_dailymail_llm.test \
  --task_args '{"max_gen_len": 50}' \
  --dataset_dir jsonl_dataset/cnn_dailymail \
  --data_loading.batch_size 1 \
  --dump_dir output_results

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm

# Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
!cat output_results/results/cnn_dailymail_llm.test.json

# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø´Ø£Ø©
!ls -la output_results/

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø®Ø§Ù…
!ls -la output_results/raw_results/cnn_dailymail_llm.test/

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ©
!mkdir -p training_data

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
!pip install -e ".[train]"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù…Ø«Ø§Ù„
import json
import os

os.makedirs('training_data/concept_dataset', exist_ok=True)

# Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ù…ÙØ§Ù‡ÙŠÙ…
training_data = [
    {
        "text": "The cat sat on the mat",
        "concepts": ["cat", "sitting", "mat", "domestic", "resting"]
    },
    {
        "text": "Artificial intelligence is transforming healthcare",
        "concepts": ["AI", "healthcare", "technology", "transformation", "innovation"]
    },
    {
        "text": "Climate change causes extreme weather events",
        "concepts": ["climate", "weather", "environment", "extreme", "causes"]
    }
]

# Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
with open('training_data/concept_dataset/train.jsonl', 'w') as f:
    for item in training_data:
        f.write(json.dumps(item) + '\n')

print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
!python -m lcm.training.train \
  --model_name "gpt2" \
  --train_data_path "training_data/concept_dataset/train.jsonl" \
  --output_dir "trained_lcm_model" \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --learning_rate 5e-5 \
  --concept_dim 256

!python -m lcm.training.train \
  --model_name "gpt2" \
  --train_data_path "training_data/concept_dataset/train.jsonl" \
  --output_dir "trained_lcm_advanced" \
  --num_train_epochs 5 \
  --per_device_train_batch_size 4 \
  --learning_rate 3e-5 \
  --warmup_steps 100 \
  --logging_steps 10 \
  --save_steps 100 \
  --concept_dim 512 \
  --max_seq_length 512 \
  --gradient_accumulation_steps 2

# Ø¥Ø°Ø§ Ù„Ø¯ÙŠÙƒ Ø¨ÙŠØ§Ù†Ø§Øª concept-labeled
!python -m lcm.training.train \
  --model_name "gpt2-medium" \
  --train_data_path "path/to/your/concept/data.jsonl" \
  --output_dir "real_trained_lcm" \
  --num_train_epochs 10 \
  --per_device_train_batch_size 8 \
  --learning_rate 2e-5 \
  --concept_dim 768

# Ù…Ø±Ø§Ù‚Ø¨Ø© ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
!tensorboard --logdir trained_lcm_model/logs

# Ø£Ùˆ Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¨Ø§Ø´Ø±Ø©
!ls -la trained_lcm_model/
!cat trained_lcm_model/training_logs.txt

# Ø¨Ø¹Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
from lcm import LCModel

model = LCModel.from_pretrained("trained_lcm_model")
print("Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")
print(f"Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {model.concept_dim}")

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
concepts = model.extract_concepts("Technology is changing the world")
print("Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:", concepts)

lcm.train

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model/lcm/train/lcm

!uv run trainer.py \
  --model_name "gpt2" \
  --train_data_path "training_data/concept_dataset/train.jsonl" \
  --output_dir "trained_lcm_advanced" \
  --num_train_epochs 5 \
  --per_device_train_batch_size 4 \
  --learning_rate 3e-5 \
  --warmup_steps 100 \
  --logging_steps 10 \
  --save_steps 100 \
  --concept_dim 512 \
  --max_seq_length 512 \
  --gradient_accumulation_steps 2

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ LCM Ø¬Ø§Ù‡Ø²
!uv run python -c "
from lcm import LCModel
model = LCModel.from_pretrained('facebook/lcm-llama-3-8B')  # Ø£Ùˆ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ø¢Ø®Ø±
print('Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø²!')

"""https://huggingface.co/facebook/layerskip-llama3-8B"""









!torchrun --standalone --nnodes=1 --nproc-per-node=2 \ -m lcm.train launcher=standalone \ +pretrain=mse \ ++trainer.data_loading_config.max_tokens=1000 \ ++trainer.output_dir="checkpoints/mse_lcm" \ +trainer.use_submitit=false















"""### https://chatgpt.com/c/68a90a47-7ce0-8329-903b-8378684a1034"""

#source .venv/bin/activate
!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.train \
  --config-path recipes/train/ \
  ... # Ø¨Ù‚ÙŠØ© Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø­Ø³Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙˆØµÙØ©

!uv run --extra data prepare_evaluation_data.py prepare_data \
  --dataset_name=cnn_dailymail \
  --output_dir=jsonl_dataset \
  --source_text_column=article \
  --target_text_column=highlights \
  --version=3.0.0

!uv run --extra data prepare_evaluation_data.py embed \
  --input_path=jsonl_dataset/cnn_dailymail/test.jsonl \
  --lang=eng_Latn \
  --output_dir=parquet_dataset/cnn_dailymail

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

!git pull origin main

!uv run --extra data scripts/prepare_wikipedia.py /content/wikidata -- \
--data_dir 20231101.en \
--split "train [0:50]" \
--batch_size 2

# Commented out IPython magic to ensure Python compatibility.
# ØªÙ†ØµÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
!git clone https://github.com/facebookresearch/large_concept_model.git
# %cd large_concept_model

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
!uv sync --extra cpu --extra data --extra eval
!uv pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu
!uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
!pip install -e ".[data,eval]"

!export USER=colab

!uv run --extra data scripts/prepare_wikipedia.py /content/test_wiki -- \
--data_dir 20231101.en \
--split "train [0:50]" \
--batch_size 2 \
launcher=local

# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
!pip install datasets pyarrow nltk

# 2. ØªØ­Ù…ÙŠÙ„ Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù…Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ (Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
from datasets import load_dataset
ds = load_dataset("wikipedia", "20231101.en", split="train[:50]")  # 50 Ù…Ù‚Ø§Ù„ ÙÙ‚Ø·

# 3. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NLTK
import nltk
nltk.download("punkt")
from nltk.tokenize import sent_tokenize

def split_sentences(example):
    example["sentences"] = sent_tokenize(example["text"])
    return {"sentences": example["sentences"]}

ds = ds.map(split_sentences, batched=False)

# 4. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

df = pd.DataFrame({
    "sentences": ds["sentences"]
})
table = pa.Table.from_pandas(df)
pq.write_table(table, "/content/simple_wiki.parquet")

print("ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ /content/simple_wiki.parquet")

from datasets import load_dataset
dataset = load_dataset("parquet", data_files="/path/to/file.parquet")
# Ø£Ùˆ
dataset = load_dataset("csv", data_files="data.csv")

#!pip install datasets pyarrow nltk
from datasets import load_dataset
# Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø© Parquet
# ds = load_dataset("wikipedia", "20231101.en", split="train[:50]", streaming=False)
# Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„ÙƒØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:

ds = load_dataset("wikipedia", "20231101.en", split="train[:50]")  # ÙÙ‚Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
import nltk
nltk.download("punkt")
from nltk.tokenize import sent_tokenize

def split_sentences(example):
    example["sentences"] = sent_tokenize(example["text"])
    return {"sentences": example["sentences"]}

ds = ds.map(split_sentences, batched=False)

# ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ CSV:
ds.to_csv("/content/simple_wiki.csv")

# Ø«Ù… ØªØ­Ù…ÙŠÙ„Ù‡:
ds2 = load_dataset("csv", data_files="/content/simple_wiki.csv", split="train")

!pip install datasets==3.6.0

from datasets import load_dataset

ds = load_dataset("wikipedia", "20231101.en", split="train[:50]")

from datasets import load_dataset

ds = load_dataset("wikipedia", "20220301.en", split="train[:50]", trust_remote_code=True)

from datasets import load_dataset

dataset = load_dataset(
    "wikipedia",
    "20220301.en",
    split="train",
    streaming=True
)
small_samples = dataset.take(20)
for example in small_samples:
    print(example["text"][:200])  # ÙŠØ¹Ø±Ø¶ Ø£ÙˆÙ„ 200 Ø­Ø±Ù ÙÙ‚Ø·

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.train \
  --config-path recipes/train/finetune \
  +pretrain=mse \
  ++trainer.output_dir="checkpoints/my_base_lcm" \
  ++trainer.data_loading_config.max_tokens=200 \
  ++trainer.data_loading_config.packing=false \
  ++trainer.checkpoint_every_n_steps=50 \
  ++trainer.save_model_every_n_steps=100

!!uv pip install torch==2.5.0+cpu torchvision==0.19.1+cpu --index-url https://download.pytorch.org/whl/cpu

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.train \
  --config-path recipes/train/finetune \
  +pretrain=mse \
  ++trainer.output_dir="checkpoints/my_base_lcm" \
  ++trainer.data_loading_config.max_tokens=200 \
  ++trainer.data_loading_config.packing=false \
  ++trainer.checkpoint_every_n_steps=50 \
  ++trainer.save_model_every_n_steps=100

!!uv pip install torch==2.5.0+cpu torchvision==0.19.1+cpu --index-url https://download.pytorch.org/whl/cpu

!uv run torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.train \
  --config-path recipes/train/finetune \
  +pretrain=mse \
  ++trainer.output_dir="checkpoints/my_base_lcm" \
  ++trainer.data_loading_config.max_tokens=200 \
  ++trainer.data_loading_config.packing=false \
  ++trainer.checkpoint_every_n_steps=50 \
  ++trainer.save_model_every_n_steps=100

!!uv pip install torch==2.5.0+cpu torchvision==0.19.1+cpu --index-url https://download.pytorch.org/whl/cpu

!!uv pip install --upgrade torchvision==0.20.1+cu124

!pip install torch==2.5.0+cpu torchvision==0.19.1+cpu --index-url https://download.pytorch.org/whl/cpu







"""https://github.com/facebookresearch/large_concept_model/issues/23"""

# clone the repo
!git clone https://github.com/facebookresearch/large_concept_model.git

cd large_concept_model

# Install GPU-enabled Fairseq2 compatible with PyTorch 2.5.1
!pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cu121

# Install other dependencies required for the project
!pip install -e ".[data,eval]" -f /content/large_concept_model

# create directory for prep_wiki data
!mkdir -p /content/large_concept_model/sample_data

# run the data preparation script
!python large_concept_model/scripts/prepare_wikipedia.py /content/large_concept_model/sample_data

# create directory for normalizer.pt file
!mkdir -p /content/large_concept_model/normalizer

# create normalizer.pt file
!python /content/large_concept_model/scripts/fit_embedding_normalizer.py --ds pretraining_data_train --save_path "/content/large_concept_model/normalizer/normalizer_train.pt" --max_nb_samples 1000000

# training script for MSE_LCM
!CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nnodes=1 --nproc-per-node=1 -m large_concept_model.lcm.train \
    launcher=standalone +pretrain=mse \
    ++trainer.data_loading_config.max_tokens=512 \
    ++trainer.output_dir="/content/drive/MyDrive/LCM/checkpoints/mse_lcm" \
    +trainer.use_submitit=false

/root/.cache/huggingface/hub/datasets--wikipedia

!python /content/large_concept_model/scripts/fit_embedding_normalizer.py --ds pretraining_data_train --save_path "/content/large_concept_model/normalizer/normalizer_train.pt" --max_nb_samples 1000000

!torchrun --standalone --nnodes=1 --nproc-per-node=1 -m large_concept_model.lcm.train \
    launcher=standalone +pretrain=mse \
    ++trainer.data_loading_config.max_tokens=512 \
    ++trainer.output_dir="/content/drive/MyDrive/LCM/checkpoints/mse_lcm" \
    +trainer.use_submitit=false

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model

!uv pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu
!uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
!pip install -e ".[data,eval]"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/large_concept_model
!python /content/large_concept_model/scripts/fit_embedding_normalizer.py --ds pretraining_data_train --save_path "/content/large_concept_model/normalizer/normalizer_train.pt" --max_nb_samples 10

